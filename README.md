# PropFirm Trading Rules Scraper

A comprehensive Playwright-based web scraping system that collects structured trading rule data from prop firm websites and exports to Google Sheets.

## ğŸ¯ Project Overview

This scraper automates the collection of trading rules from 13+ prop firm websites, normalizes the data, converts currencies to USD, and exports everything to a Google Sheet for analysis.

### Key Features

- **Automated Web Scraping**: Uses Playwright for robust browser automation
- **Multi-Site Support**: Configurable extractors for different prop firm websites
- **Data Normalization**: Converts all monetary values to USD with strict enum classification
- **Google Sheets Integration**: Automatically populates and overwrites Google Sheets
- **Error Handling**: Gracefully handles login requirements and missing data
- **Fallback Mechanisms**: Uses search fields and chatbots when needed

## ğŸ“ Project Structure

```
propfirm_scraper/
â”‚
â”œâ”€â”€ main.py                 # Main execution script
â”‚
â”œâ”€â”€ core/                   # Core utilities
â”‚   â”œâ”€â”€ browser.py          # Playwright browser management
â”‚   â”œâ”€â”€ logger.py           # Logging configuration
â”‚   â”œâ”€â”€ utils.py            # Data extraction utilities
â”‚   â””â”€â”€ currency_converter.py  # Currency conversion
â”‚
â”œâ”€â”€ extractors/             # Website-specific extractors
â”‚   â”œâ”€â”€ base_extractor.py   # Abstract base class
â”‚   â”œâ”€â”€ apex.py            # Apex Trader Funding
â”‚   â”œâ”€â”€ tradeify.py        # Tradeify
â”‚   â””â”€â”€ ...                # Other extractors
â”‚
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ sites.yaml         # Website configurations
â”‚   â”œâ”€â”€ enums.py           # Strict enum definitions
â”‚   â””â”€â”€ schema.py          # Data schemas
â”‚
â”œâ”€â”€ exporters/              # Data export modules
â”‚   â””â”€â”€ google_sheets.py   # Google Sheets exporter
â”‚
â”œâ”€â”€ fallback/               # Fallback mechanisms
â”‚   â””â”€â”€ chatbot.py         # Chatbot integration
â”‚
â”œâ”€â”€ data/                   # Data storage
â”‚   â””â”€â”€ raw/               # Raw JSON files
â”‚
â””â”€â”€ logs/                   # Log files
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
playwright install
```

### 2. Set Up Google Sheets API

1. Create a Google Cloud project
2. Enable Google Sheets API
3. Create a service account and download JSON credentials
4. Place credentials in `service_account/` folder
5. Share your Google Sheet with the service account email

### 3. Configure Target Sheet

Update the sheet ID in `main.py`:
```python
self.sheet_id = "YOUR_GOOGLE_SHEET_ID"
```

### 4. Run the Scraper

```bash
cd propfirm_scraper
python main.py
```

## ğŸ“Š Data Structure

Each row in the output represents one firm + account size combination:

### Metadata
- Firm Name
- Account Size (original)
- Account Size (USD)
- Website URL
- Broker
- Platform
- Last Updated
- Status

### Evaluation Phase
- Evaluation Target (USD)
- Evaluation Max Drawdown (USD)
- Evaluation Daily Loss (USD)
- Evaluation Drawdown Type (ENUM)
- Evaluation Min Days
- Evaluation Consistency (BOOLEAN)

### Funded Phase
- Funded Max Drawdown (USD)
- Funded Daily Loss (USD)
- Funded Drawdown Type (ENUM)

### Payout
- Profit Split (%)
- Payout Frequency (ENUM)
- Min Payout (USD)

### Fees
- Evaluation Fee (USD)
- Reset Fee (USD)

## ğŸ”§ Configuration

### Adding New Websites

1. Add site configuration to `config/sites.yaml`
2. Create new extractor class inheriting from `BaseExtractor`
3. Implement required methods for data extraction

### Enum Values

Strict enums are defined in `config/enums.py`:

- **Drawdown Type**: TRAILING, STATIC, EOD, HYBRID
- **Payout Frequency**: WEEKLY, BIWEEKLY, MONTHLY, ON_DEMAND
- **Status**: OK, MISSING_DATA, LOGIN_REQUIRED, FAILED

## ğŸ§ª Testing

Run the setup verification:

```bash
python test_setup.py
```

This will test:
- Module imports
- Currency conversion
- Utility functions
- Google Sheets configuration
- YAML configuration loading

## ğŸ“ Logging

Logs are saved to `propfirm_scraper/logs/` with timestamps. The logger captures:
- Scraping progress
- Errors and warnings
- Data extraction details
- Export status

## ğŸ”„ Current Status

**Phase 1 Complete**: âœ… Project setup and foundation
- Complete folder structure
- Core utilities and browser management
- Google Sheets integration
- Configuration system
- Base extractor framework

**Next Phase**: Website-specific extractors implementation

## ğŸ¯ Target Websites

Currently configured for 13 prop firm websites:
- Apex Trader Funding
- Lucid Trading
- Tradeify
- My Funded Futures
- Funded Next
- Alpha Futures
- Top One Futures
- Blue Guardian Futures
- The Trading Pit
- Legends Trading
- E8 Markets
- Take Profit Trader
- Trade Day

## ğŸ”’ Security Notes

- Service account credentials are stored locally
- No sensitive data is logged
- Browser runs in sandboxed environment
- Respectful scraping with delays between requests

## ğŸ“ˆ Output

The scraper generates:
- **Google Sheet**: Live data with all extracted rules
- **Raw JSON files**: Backup data for debugging
- **Log files**: Detailed execution logs
- **Summary report**: Status and statistics
